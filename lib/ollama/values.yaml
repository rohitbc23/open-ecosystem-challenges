# Minimal Ollama configuration for Codespaces
# Using Qwen2 0.5B to fit within resource constraints

ollama:
  # Pre-pull model on startup
  models:
    pull:
      - qwen2.5:0.5b

# Resource limits suitable for GitHub Codespaces (2-core, 8GB)
# Low request allows other pods to schedule, high limit allows bursting during inference
resources:
  requests:
    memory: 2Gi
    cpu: 500m
  limits:
    memory: 3Gi
    cpu: 1500m

# Expose service for other pods to connect
service:
  type: NodePort
  port: 11434
  nodePort: 30105
